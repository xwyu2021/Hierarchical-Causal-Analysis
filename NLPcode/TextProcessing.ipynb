{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "############## (1) phrases extraction ######################\n",
    "############################################################\n",
    "\n",
    "######## install packages ########\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en\n",
    "!{sys.executable} -m pip install textacy\n",
    "import spacy\n",
    "import re\n",
    "import textacy\n",
    "from spacy.matcher import Matcher\n",
    "import nltk\n",
    "import pprint\n",
    "from nltk import RegexpParser\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, sent_tokenize\n",
    "from nltk import Tree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## functions ############\n",
    "nlp = spacy.load(\"en\")\n",
    "### no repetition version\n",
    "def get_continuous_chunks(text, chunk_func=ne_chunk):\n",
    "    chunked = chunk_func(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                current_chunk = [] \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return continuous_chunk\n",
    "\n",
    "def get_simple_chunks(text,chunk_func=ne_chunk):\n",
    "    chunked = chunk_func(pos_tag(word_tokenize(text)))\n",
    "    simple_chunk = []\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree:\n",
    "            simple_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "    return simple_chunk\n",
    "\n",
    "def preprocess_s(sentence,myrule):\n",
    "    token = word_tokenize(sentence)\n",
    "    tags = pos_tag(token)\n",
    "    cp = RegexpParser(myrule)\n",
    "    result = cp.parse(tags)\n",
    "    return result\n",
    "def preprocess(document):\n",
    "    sentences = sent_tokenize(document)\n",
    "    token = [word_tokenize(sent) for sent in sentences]\n",
    "    tags = [pos_tag(sent) for sent in token]\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    cp = RegexpParser(grammar)\n",
    "    result = [cp.parse(sent) for sent in tags]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## example sentence ############################\n",
    "NP = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN.*>+} # noun phrase\n",
    "  PP: {<IN><NP>}               # prepositional phrase\n",
    "  VP: {<MD>?<VB.*><NP|PP>}     # verb phrase\n",
    "  CLAUSE: {<NP><VP>}           # full clause\n",
    "  ADJ: {<VB|VBD|VBN|VBZ><VBN|VBG>}\n",
    "  NVB:{<NP><VB|VBD>}\n",
    "  \"\"\"\n",
    "cp = RegexpParser(NP)\n",
    "mysent = \"Weather or environment or pollution caused coating defect in the low-voltage bushing showing blue phase - topping-up oil.\"\n",
    "print(pos_tag(word_tokenize(mysent)))\n",
    "print(get_continuous_chunks(mysent, cp.parse))\n",
    "\n",
    "NP1 = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN.*>+} # noun phrase\n",
    "  \n",
    "  VP: {<MD>?<VB.*><NP|PP>}     # verb phrase\n",
    "  CLAUSE: {<NP><VP>}           # full clause\n",
    "  ADJ: {<VB|VBD|VBN|VBZ><VBN|VBG>}\n",
    "  NVB:{<NP><VB|VBD>}\n",
    "  \"\"\"\n",
    "cp1 = RegexpParser(NP1)\n",
    "print(get_continuous_chunks(mysent, cp1.parse))\n",
    "print(get_simple_chunks(mysent,cp1.parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### example 2######################\n",
    "doc = \"OIL LEAK - Bleed Valve to be replaced: Bleed Valve has been damaged in the past when removed by the use of stilsons, Bleed Valve to be replaced.\"\n",
    "NP = \"NP: {(<V\\w+>|<NN\\w?>)+.*<NN\\w?>}\"\n",
    "chunks = preprocess_s(doc,NP)\n",
    "print(chunks)\n",
    "chunks[0].draw()\n",
    "get_continuous_chunks(doc, cp.parse)\n",
    "mysent = \"Weather or environment or pollution caused coating defect in the low-voltage bushing showing blue phase - low-voltage bushing and high-voltage bushing requires painting.\"\n",
    "myone = get_continuous_chunks(\"Weather or environment or pollution caused coating defect in the low-voltage bushing showing blue phase - low-voltage bushing and high-voltage bushing requires painting.\", cp1.parse)\n",
    "print(myone)\n",
    "print(mytwo)\n",
    "mytwo = causalmentionsent(mysent)\n",
    "myha = mytwo[1][1].lower()\n",
    "phrase = myone[4].lower()\n",
    "print(phrase in myha) \n",
    "match = phrase in myha\n",
    "if match:\n",
    "    print(phrase)\n",
    "\n",
    "here = pd.DataFrame({\"cause\":[],\"effect\":[]})\n",
    "for i in range(len(mytwo)):\n",
    "    aa = find_phrases(myone,mytwo[i][0])\n",
    "    bb = find_phrases(myone,mytwo[i][1])\n",
    "    thisisit = combos(aa,bb)\n",
    "    if(all([len(b) == 1 for b in bb]) == True):\n",
    "        thisisit = combos(aa,word_tokenize(bb))\n",
    "    if(all([len(b) == 1 for b in aa]) == True):\n",
    "        thisisit = combos(word_tokenize(aa),bb)\n",
    "    if(all([len(b) == 1 for b in bb]) == True and all([len(b) == 1 for b in aa]) == True):\n",
    "        thisisit = combos(word_tokenize(aa),word_tokenize(bb))\n",
    "        \n",
    "    for combo in thisisit:\n",
    "      new_row = {\"cause\":combo[0],\"effect\":combo[1]}\n",
    "      here = here.append(new_row, ignore_index=True)\n",
    "print(here.head())\n",
    "print(combos(find_phrases(myone,mytwo[1][0]),find_phrases(myone,mytwo[1][1])))\n",
    "combos(find_phrases(myone,mytwo[i][0]),word_tokenize(find_phrases(myone,mytwo[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "###########  (2) Causal Mentions   ####################\n",
    "#######################################################\n",
    "####\n",
    "#### import packages\n",
    "####\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# causal rules #####################\n",
    "p_after = re.compile(r\"After ([a-zA-Z\\'\\- ]+?),\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_because = re.compile(r\"([\\w\\'\\- ]+?)because(?!\\sof) ([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_due_to_1 = re.compile(r\"([\\w\\'\\- ]+)[was|were|is|are]?due to\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_due_to_2 = re.compile(r\"([\\w\\'\\- ]+),\\sdue to\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\") ##, needed? check later\n",
    "p_because_of_1 = re.compile(r\"Because of ([\\w\\'\\- ]+?),\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_because_of_2 = re.compile(r\"([\\w\\'\\- ]+)because of\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_cause = re.compile(r\"([\\w\\'\\- ]+)\\scause(?!\\sby)\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_caused = re.compile(r\"([\\w\\'\\- ]+)\\scaused(?!\\sby)\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_causes = re.compile(r\"([\\w\\'\\- ]+)\\scauses(?!\\sby)\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_caused_by = re.compile(r\"([\\w\\'\\- ]+)[was|were|is|are]?caused by\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_lead_to = re.compile(r\"([\\w\\'\\- ]+)lead to ([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "## dash,colon\n",
    "p_require = re.compile(r\"([\\w\\'\\- ]+)\\srequire[s|d]?([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_so = re.compile(r\"([\\S ]+?)so(?!\\sof) ([\\S ]+)\\s?[\\.,\\?]\")\n",
    "p_result_in = re.compile(r\"([\\w\\'\\- ]+)\\sresult\\sin\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_results_in = re.compile(r\"([\\w\\'\\- ]+)\\sresults\\sin\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_resulted_in = re.compile(r\"([\\w\\'\\- ]+)\\sresulted\\sin\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_as_a_consequence=re.compile(r\"([\\S ]+?)as a consequence(?!\\sof) ([\\S ]+)\\s?[\\.,\\?]\")\n",
    "p_as_a_result=re.compile(r\"([\\S ]+?)as a result(?!\\sof) ([\\S ]+)\\s?[\\.,\\?]\")\n",
    "#p_for_reason = re.compile(r\"([\\S ]+?)for reason(?!\\sthat) ([\\S ]+)\\s?[\\.,\\?]\")\n",
    "p_dash = re.compile(r\"([\\w\\'\\- ]+)\\s\\-\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "p_colon = re.compile(r\"([\\w\\'\\- ]+)\\:\\s([\\w\\'\\- ]+)\\s?[\\.,\\?]\")\n",
    "\n",
    "triggers = [\"After\",\" because of\",\" because\",\" is caused by\",\" was caused by\",\" are caused by\",\" were caused by\",\n",
    "           \" caused\",\" causes\",\" cause\",\" is due to\",\" are due to\",\" was due to\",\" were due to\",\" result in\",\" resulted in\",\n",
    "           \" results in\",\" as a consequence of\",\" as a consequence\",\" as a result of\",\" as a result\",\" so that\",\" so\",\n",
    "           \" required\",\" requires\",\" require\"]\n",
    "# Some notes: [] indicate a character class, which matches only a single character, e.g. gr[ae]y does not match graay\n",
    "#             () for grouping\n",
    "#             ^ negated character classes, if not right after the opening bracket, its just a character\n",
    "#             ? optional items, e.g. Nov(ember)? matches Nov and November, colou?r matches colour and color\n",
    "#            ?,*,+ repeating character class, ([0-9])\\1+ matches 222, repeated same number\n",
    "#             + 1+1=2 matches to 111=2 repeat the preceding token once or more\n",
    "#             * repeat the preceding token >=0 times\n",
    "#             (?!)negative lookahead, the regex only match when the capturing group not match\n",
    "#             \\d shorthand for [0-9]\n",
    "#             \\w shorthand for word character [A-Za-z0-9_]\n",
    "#             \\s whitespace character includes [ \\t\\r\\n\\f] a space, a tab, a carriage return, a line feed, or a form feed\n",
    "#             \\D equivalently [^\\d]\n",
    "#             \\W equivalently [^\\w]\n",
    "#             \\S equivalently [^\\s]\n",
    "#             [^\\d\\s] matches any character neither a digit nor whitespace\n",
    "#             [\\D\\S] mathces any character that is either not a digit or is not whitespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################## functions to pick up the causal mentions from causal rules\n",
    "def test():\n",
    "    \n",
    "   # line = r\"after wesd Afghan vote, comp-laints because lead-to that' of fraud, surfacev ? jadsklfj?\"\n",
    "    line = r\"low oil caused outage.\"\n",
    "   # match = p_because.search(line)\n",
    "    match = p_caused.search(line)\n",
    "    if match:\n",
    "        print(\"Match Rex ...\")\n",
    "        print(match.group(0))\n",
    "        print(match.group(1))\n",
    "        print(match.group(2))\n",
    "    else:\n",
    "        print(\"Cannot Match ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## for a single sentence\n",
    "\n",
    "## for a document\n",
    "\n",
    "## segment sentences : doctype = 1 represent csv, 0 represent txt\n",
    "##                     document is the dataframe, converted from csv\n",
    "def sentencesplittxt(doctype,document,sentencefile):\n",
    "    if doctype == 0:\n",
    "        input = open(document, 'r')\n",
    "        all_the_text = input.read()\n",
    "        input.close()\n",
    "    \n",
    "        text = all_the_text.rstrip() ## remove whitespace at the end of the string from all texts\n",
    "        text = all_the_text.lstrip() ## remove whitespace at the beginning of the string \n",
    "        text = \" \".join(text.split()) ## split a string into a list s.t. each word is a list item, then join them into a string using a space as a separator\n",
    "    \n",
    "    #Math the end of sentences\n",
    "        end = re.compile(r\"[.!?.]+\")\n",
    "        sentences = re.split(end, text)[:-1]\n",
    "        output = open(sentencefile, 'w')\n",
    "        for sentence in sentences:\n",
    "            output.writelines(sentence+'.'+'\\n')\n",
    "\n",
    "        output.close()\n",
    "        \n",
    "\n",
    "## causal mention extraction for a single document (a list) with multiple sentences \n",
    "##                          length = len(segment), number of sentences\n",
    "def causalmentiondoc(document,length):\n",
    "    listofpair=[]\n",
    "    i = 0\n",
    "    \n",
    "    while i < length:\n",
    "        causeeffect=[]\n",
    "        # (1) after\n",
    "        match = p_after.search(str(document[i])) \n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (2) because    \n",
    "        match = p_because.search(str(document[i])) \n",
    "        if match:\n",
    "            causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (3) due to 1\n",
    "        match = p_due_to_1.search(str(document[i])) \n",
    "        if match:\n",
    "            causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (4) due to 2\n",
    "        match = p_due_to_2.search(str(document[i])) \n",
    "        if match:\n",
    "            causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (5) because of 1\n",
    "        match = p_because_of_1.search(str(document[i])) \n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (6) because of 2\n",
    "        match = p_because_of_2.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (7) cause\n",
    "        match = p_cause.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        match = p_caused.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        match = p_causes.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (8) caused by\n",
    "        match = p_caused_by.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (9) lead to\n",
    "        match = p_lead_to.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (10) require\n",
    "        match = p_require.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (11) so\n",
    "        match = p_so.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (12) result in\n",
    "        match = p_result_in.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (13) results in\n",
    "        match = p_results_in.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (14) resulted in\n",
    "        match = p_resulted_in.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (15) as a consequence\n",
    "        match = p_as_a_consequence.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (16) as a result\n",
    "        match = p_as_a_result.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (17) dash\n",
    "        match = p_dash.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (18) colon\n",
    "        match = p_colon.search(str(document[i]))\n",
    "        if match:\n",
    "            causeeffect.append((match.group(1),match.group(2)))\n",
    "            \n",
    "        listofpair.append(causeeffect)\n",
    "        i += 1    \n",
    "        \n",
    "    #listofpair = list(filter(None,listofpair))\n",
    "    return listofpair  \n",
    "\n",
    "\n",
    "### if only for sentences:\n",
    "def causalmentionsent(sent):\n",
    "   \n",
    "    causeeffect=[]\n",
    "        # (1) after\n",
    "    match = p_after.search(str(sent)) \n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (2) because    \n",
    "    match = p_because.search(str(sent)) \n",
    "    if match:\n",
    "         causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (3) due to 1\n",
    "    match = p_due_to_1.search(str(sent)) \n",
    "    if match:\n",
    "        causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (4) due to 2\n",
    "    match = p_due_to_2.search(str(sent)) \n",
    "    if match:\n",
    "        causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (5) because of 1\n",
    "    match = p_because_of_1.search(str(sent)) \n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (6) because of 2\n",
    "    match = p_because_of_2.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (7) cause\n",
    "    match = p_cause.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "    match = p_caused.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "    match = p_causes.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (8) caused by\n",
    "    match = p_caused_by.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(2),match.group(1)))\n",
    "        # (9) lead to\n",
    "    match = p_lead_to.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (10) require\n",
    "    match = p_require.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (11) so\n",
    "    match = p_so.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (12) result in\n",
    "    match = p_result_in.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (13) results in\n",
    "    match = p_results_in.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (14) resulted in\n",
    "    match = p_resulted_in.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (15) as a consequence\n",
    "    match = p_as_a_consequence.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (16) as a result\n",
    "    match = p_as_a_result.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (17) dash\n",
    "    match = p_dash.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))\n",
    "        # (18) colon\n",
    "    match = p_colon.search(str(sent))\n",
    "    if match:\n",
    "        causeeffect.append((match.group(1),match.group(2)))  \n",
    "        \n",
    "   \n",
    "    return causeeffect     \n",
    "        \n",
    "def causalmentions(doctype,document,sentencefile):\n",
    "    end = re.compile(r\"[.!?.]+\")\n",
    "    output = open(sentencefile, 'w')\n",
    "    if doctype == 1:\n",
    "        listofdocs = df[\"logs\"].tolist()\n",
    "        seglength = []\n",
    "        mentions = []\n",
    "        for entry in listofdocs:\n",
    "            text = entry.rstrip()\n",
    "            text = \" \".join(text.split())\n",
    "            sentences = re.split(end, text)[:-1]\n",
    "            seglength.append(len(sentences))\n",
    "            segment = []\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                segment.append(sentence+'.')\n",
    "            \n",
    "            mentions.append(causalmentiondoc(segment,len(sentences)))\n",
    "    \n",
    "    return mentions\n",
    "                \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "def phrase_in(phrase,mention):\n",
    "    phrase = phrase.lower()\n",
    "    mention = mention.lower()\n",
    "    return phrase in mention\n",
    "def find_phrases(phrases,mention):\n",
    "    gamma = []\n",
    "    mention = mention.lower()\n",
    "    for phrase in phrases:\n",
    "        phrase = phrase.lower()\n",
    "      #  print(phrase in mention) \n",
    "        match = phrase in mention\n",
    "        if match:\n",
    "            gamma.append(phrase)\n",
    "            #gamma +=(phrase,)\n",
    "    if len(gamma) == 0:\n",
    "        if(word_tokenize(mention) == mention):\n",
    "            gamma.append(word_tokenize(mention))\n",
    "        else:\n",
    "            gamma = mention\n",
    "    return gamma   \n",
    "\n",
    "def combos(lista,listb):\n",
    "    combos = []\n",
    "    for i in lista:\n",
    "        for j in listb:\n",
    "            combos.append([i,j])\n",
    "    return combos\n",
    "        \n",
    "#def causalphrases(phrases,causalmentions):\n",
    "    \n",
    "## for a file of documents (preprocess to pick sentences/docs with trigger words)\n",
    "#def causalmentions_file(sentencefile,causalityfile):\n",
    "    \n",
    "    ## define terms \n",
    "#    input = open(sentencefile, 'r')\n",
    "#    sentences = input.readlines()\n",
    "#    input.close()\n",
    "#    os.remove(sentencefile)\n",
    "#    output = open(causalityfile,'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## eample 3 ###########\n",
    "hard = \"The oil leak caused the machine to stop working. Need to change the tank and add 2 litres oil.\"\n",
    "hardtext = hard.strip()\n",
    "hardtext = \" \".join(hardtext.split())\n",
    "\n",
    "end = re.compile(r\"[.!?.]+\")\n",
    "hardtext = re.split(end, hardtext)[:-1]\n",
    "hardtext\n",
    "#for sent in hardtext:\n",
    "#    print(sent+'.'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ If the data is csv rather than txt\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'./Documents/PhD paper 2/data/texts2.csv')\n",
    "listofdocs = df[\"logs\"].tolist()\n",
    "tryresult = []\n",
    "phrasechunks = []\n",
    "step3 = []\n",
    "df = pd.DataFrame(columns=[\"cause\",\"effect\",\"doc_ind\",\"sentence_ind\"])\n",
    "document_index = 0\n",
    "for entry in listofdocs:\n",
    "    document_index += 1\n",
    "    text = entry.rstrip()\n",
    "    text = \" \".join(text.split())\n",
    "    sentences = re.split(end, text)[:-1]\n",
    "    segment = []\n",
    "    phrases = []\n",
    "    sentc = []\n",
    "    sentence_index = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_index += 1\n",
    "        segment.append(sentence+'.')\n",
    "        ## betaresult: causal mentions in a single sentence\n",
    "        betaresult = causalmentionsent(sentence+'.')\n",
    "       # mention = []\n",
    "       # multimention = []\n",
    "        ## remove trigger words \n",
    "        \n",
    "        for word in triggers:\n",
    "            sentence = sentence.replace(word,\",\")\n",
    "        \n",
    "        ## alpharesult: phrases parsed by rules in a single sentence with triggers removed\n",
    "        alpharesult = get_continuous_chunks(sentence+'.', cp.parse)\n",
    "        ## phrases: the phrases parsed for a document\n",
    "        phrases.append(get_continuous_chunks(sentence+'.', cp.parse))\n",
    "      \n",
    "        if len(betaresult) > 0:\n",
    "            i = 0\n",
    "            while i < len(betaresult): ## when there are more than one pair of causal mentions extracted from one sentence\n",
    "                ## for ith pair of causal mentions in a single sentence\n",
    "                ## beta is one of the mention in the ith pair for a single sentence\n",
    "                #for beta in betaresult[i]: \n",
    "                    ## parsedmention is the mention that is further refined by the causal phrases alpharesult\n",
    "                #    parsedmention = find_phrases(alpharesult,beta)\n",
    "                    ## mention\n",
    "                 #   mention.append(parsedmention)\n",
    "            \n",
    "                tempcause = find_phrases(alpharesult,betaresult[i][0])\n",
    "                tempeffect = find_phrases(alpharesult,betaresult[i][1])\n",
    "                sentencecombo = combos(tempcause,tempeffect)\n",
    "                \n",
    "                for combo in sentencecombo:\n",
    "                    new_row = {\"cause\":combo[0],\"effect\":combo[1],\"doc_ind\":document_index,\"sentence_ind\":sentence_index}\n",
    "                    df = df.append(new_row, ignore_index=True)\n",
    "                \n",
    "                i += 1\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df)\n",
    "df.to_csv(r'./Documents/PhD paper 2/data/cmoftexts2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.dom.minidom as readxml\n",
    "def xmltodf(path):\n",
    "    myfile = readxml.parse(path)\n",
    "    ## we extract the events and their indices\n",
    "    events = myfile.getElementsByTagName(\"event\")\n",
    "    event = []\n",
    "    eventid = []\n",
    "    for e in events:\n",
    "        event.append(e.attributes[\"string\"].value)\n",
    "        eventid.append(e.attributes[\"eiid\"].value)\n",
    "    ## now extract the times and their indices\n",
    "    print(len(event))\n",
    "    times = myfile.getElementsByTagName(\"timex\")\n",
    "    time = []\n",
    "    timeid = []\n",
    "    for t in times:\n",
    "        time.append(t.attributes[\"text\"].value)\n",
    "        timeid.append(t.attributes[\"tid\"].value)\n",
    "    ## now extract event pairs\n",
    "    myitem = myfile.getElementsByTagName(\"tlink\")\n",
    "    event1 = []\n",
    "    event2 = []\n",
    "    relation = []\n",
    "    for item in myitem:\n",
    "        event1.append(item.attributes[\"event1\"].value)\n",
    "        event2.append(item.attributes[\"event2\"].value)\n",
    "        relation.append(item.attributes[\"relation\"].value)\n",
    "    ## now remove those pairs with times\n",
    "    i = 0\n",
    "    subset = []\n",
    "    while i < len(event1):\n",
    "        exist1 = event1[i] in timeid\n",
    "        exist2 = event2[i] in timeid\n",
    "        if exist1 or exist2:\n",
    "            subset.append(False)\n",
    "        else:\n",
    "            subset.append(True)\n",
    "        i += 1\n",
    "    event1 = [x for x,y in zip(event1,subset) if y]\n",
    "    event2 = [x for x,y in zip(event2,subset) if y]\n",
    "    relation = [x for x,y in zip(relation,subset) if y]\n",
    "    ## now replace the indices by the texts\n",
    "    i = 0\n",
    "    verb1 = []\n",
    "    verb2 = []\n",
    "    while i < len(event1):\n",
    "        j = 0\n",
    "        for j in range(len(event)):\n",
    "            if event1[i] == eventid[j]:\n",
    "                verb1.append(event[j])\n",
    "            if event2[i] == eventid[j]:\n",
    "                verb2.append(event[j])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    mydata = {\"event1\":verb1,\"event2\":verb2,\"relation\":relation}\n",
    "    df = pd.DataFrame(data = mydata)\n",
    "    return(df)\n",
    "\n",
    "\n",
    "## create a cause-effect dataframe from caevodataframe(with document index):\n",
    "def caevodf(caevoresult):\n",
    "    ## vague relation not meaningful\n",
    "    hdf = caevoresult[caevoresult[\"relation\"] != \"VAGUE\"]\n",
    "    i = 0\n",
    "    cause = []\n",
    "    effect = []\n",
    "    doc_ind = []\n",
    "    while i < len(hdf[\"relation\"]):\n",
    "        j = list(hdf.index)[i]\n",
    "        if hdf[\"relation\"][j] == \"BEFORE\":\n",
    "            cause.append(hdf[\"event1\"][j])\n",
    "            effect.append(hdf[\"event2\"][j])\n",
    "           # doc_ind.append(hdf[\"doc_ind\"][j])\n",
    "            doc_ind.append(0)\n",
    "        elif hdf[\"relation\"][j] == \"AFTER\":\n",
    "            cause.append(hdf[\"event2\"][j])\n",
    "            effect.append(hdf[\"event1\"][j])\n",
    "           # doc_ind.append(hdf[\"doc_ind\"][j])\n",
    "            doc_ind.append(0)\n",
    "        else: ## simultaneous or included\n",
    "            cause.append(hdf[\"event1\"][j])\n",
    "            effect.append(hdf[\"event2\"][j])\n",
    "           # doc_ind.append(hdf[\"doc_ind\"][j])\n",
    "            doc_ind.append(0)\n",
    "            cause.append(hdf[\"event2\"][j])\n",
    "            effect.append(hdf[\"event1\"][j])\n",
    "           # doc_ind.append(hdf[\"doc_ind\"][j])\n",
    "            doc_ind.append(0)\n",
    "        i += 1\n",
    "    caevodata = {\"cause\":cause,\"effect\":effect,\"doc_ind\":doc_ind}\n",
    "    mydf = pd.DataFrame(data = caevodata)\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xmldf = xmltodf(\"./caevo-master/src/test/resources/conservator.txt.info.xml\")\n",
    "xmldf[\"doc_ind\"] = 0\n",
    "xmldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = caevodf(xmldf)\n",
    "print(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filterdf = pd.DataFrame(data={\"cause\":df[\"cause\"],\"effect\":df[\"effect\"],\"doc_ind\":df[\"doc_ind\"],\"sentence_ind\":df[\"sentence_ind\"]})\n",
    "ind = 0\n",
    "for text in df[\"cause\"]:\n",
    "    filterdf[\"cause\"][ind] = rm_stop(text)\n",
    "    ind += 1\n",
    "ind = 0\n",
    "for text in df[\"effect\"]:\n",
    "    filterdf[\"effect\"][ind] = rm_stop(text)\n",
    "    ind += 1\n",
    "df.head()\n",
    "filterdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagdf = pd.DataFrame(data={\"cause\":df[\"cause\"],\"effect\":df[\"effect\"],\"doc_ind\":df[\"doc_ind\"],\"sentence_ind\":df[\"sentence_ind\"]})\n",
    "tagdf = wntagset(tagdf,df,filterdf)\n",
    "tagdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### orderevent function when ignoring document index\n",
    "\n",
    "def orderedevent2(caevodf,filterdf):\n",
    "    ## lemmatize the caevodf:\n",
    "    i = 0\n",
    "    acause = []\n",
    "    aeffect = []\n",
    "    for i in list(caevodf.index):\n",
    "        thisc = lemmatizer.lemmatize(caevodf[\"cause\"][i],\"v\")\n",
    "        thise = lemmatizer.lemmatize(caevodf[\"effect\"][i],\"v\")\n",
    "        yes1 = \"cause\" in thisc or \"cause\" in thise\n",
    "        yes2 = \"require\" in thisc or \"require\" in thise\n",
    "        agree = yes1 or yes2\n",
    "        if agree == False:\n",
    "            acause.append(thisc)\n",
    "            aeffect.append(thise)\n",
    "    creatdf = {\"cause\":acause,\"effect\":aeffect}\n",
    "    lemmacaevodf = pd.DataFrame(data = creatdf)\n",
    "    addcause = []\n",
    "    addeffect = []\n",
    "    adddoc = []\n",
    "    addsent = []\n",
    "    \n",
    "    \n",
    "    for k in list(lemmacaevodf.index):\n",
    "        add = False\n",
    "        for h in list(filterdf.index):\n",
    "            exist1 = lemmacaevodf[\"cause\"][k] in filterdf[\"cause\"][h]\n",
    "            exist2 = lemmacaevodf[\"cause\"][k] in filterdf[\"effect\"][h]\n",
    "            exist3 = lemmacaevodf[\"effect\"][k] in filterdf[\"cause\"][h]\n",
    "            exist4 = lemmacaevodf[\"effect\"][k] in filterdf[\"effect\"][h]\n",
    "            cond1 = exist1 and exist3 ## cause-effect (caevo) exist in a cause\n",
    "            cond2 = exist1 and exist4 ## cause-effect exist\n",
    "            cond3 = exist2 and exist3 ## cause-effect exist as effect-cause\n",
    "            cond4 = exist2 and exist4 ## cause-effect exist in an effect\n",
    "            existed = cond1 or cond2 or cond3 or cond4\n",
    "            add = add or existed\n",
    "        if add == False: ## if any of them is true, then the pair already exist\n",
    "                    ## we should not change the existed pairs, otherwise we add this pair\n",
    "            addcause.append(lemmacaevodf[\"cause\"][k])\n",
    "            addeffect.append(lemmacaevodf[\"effect\"][k])\n",
    "            adddoc.append(\"NA\")\n",
    "            addsent.append(\"NA\")\n",
    "            \n",
    "    adddata = {\"cause\":addcause,\"effect\":addeffect,\"doc_ind\":adddoc,\"sentence_ind\":addsent}\n",
    "    adddf = pd.DataFrame(data = adddata)\n",
    "\n",
    "    outdf = filterdf.append(adddf)\n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmacdf = orderedevent2(cdf,filterdf)\n",
    "lemmacdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lemmacdf.shape)\n",
    "print(df.shape)\n",
    "lemmacdf[lemmacdf.doc_ind == \"NA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmacdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add these new caevo pairs to tagdf\n",
    "nowcause = []\n",
    "noweffect = []\n",
    "nowdoc = []\n",
    "nowsent =[]\n",
    "newpart = lemmacdf[lemmacdf.doc_ind == \"NA\"]\n",
    "i=0\n",
    "while i < 1129:\n",
    "    if i < 1117:\n",
    "        nowcause.append(tagdf[\"cause\"][i])\n",
    "        noweffect.append(tagdf[\"effect\"][i])\n",
    "        nowdoc.append(tagdf[\"doc_ind\"][i])\n",
    "        nowsent.append(tagdf[\"sentence_ind\"][i])\n",
    "    else:\n",
    "        nowcause.append(tagtuple_convert(pos_tag(word_tokenize(newpart[\"cause\"][i-1117]))))\n",
    "        noweffect.append(tagtuple_convert(pos_tag(word_tokenize(newpart[\"effect\"][i-1117]))))\n",
    "        nowdoc.append(\"NA\")\n",
    "        nowsent.append(\"NA\")\n",
    "    i += 1\n",
    "nowdata = {\"cause\":nowcause,\"effect\":noweffect,\"doc_ind\":nowdoc,\"sentence_ind\":nowsent}\n",
    "newtagdf = pd.DataFrame(data=nowdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## adjust index of lemmacdf\n",
    "newlemmacdf = pd.DataFrame(data = {\"cause\":list(lemmacdf[\"cause\"]),\n",
    "                                  \"effect\":list(lemmacdf[\"effect\"]),\n",
    "                                  \"doc_ind\":list(lemmacdf[\"doc_ind\"]),\n",
    "                                  \"sentence_ind\":list(lemmacdf[\"sentence_ind\"])},\n",
    "                          index = range(1129))\n",
    "nouns = pairnouns(picknouns(newlemmacdf,newtagdf))\n",
    "print(nouns)\n",
    "hypers = hypernyms(nouns)\n",
    "hypers.to_csv(r'./Documents/PhD paper 2/data/hyperstext2.csv')\n",
    "print(hypers)\n",
    "verbs = pickverbs(newlemmacdf,newtagdf)\n",
    "classes = verbclass(verbs)\n",
    "classes.to_csv(r'./Documents/PhD paper 2/data/classestext2.csv')\n",
    "newlemmacdf.to_csv(r'./Documents/PhD paper 2/data/cmcaevotext2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(filterdf[\"cause\"][1])\n",
    "print(df[\"cause\"][1])\n",
    "print(subset_sentence(filterdf[\"cause\"][1],df[\"cause\"][1]))\n",
    "print([pos_tag(word_tokenize(df[\"cause\"][1]))[i] for i in subset_sentence(filterdf[\"cause\"][1],df[\"cause\"][1])])\n",
    "haha = [pos_tag(word_tokenize(df[\"cause\"][1]))[i] for i in subset_sentence(filterdf[\"cause\"][1],df[\"cause\"][1])]\n",
    "print(tagtuple_convert(haha)) \n",
    "print(lemmacdf[\"cause\"][1])\n",
    "print(lemmacdf[\"effect\"][1])\n",
    "print(cdf)\n",
    "print(tagtuple_convert(pos_tag(word_tokenize(cdf[\"cause\"][0]))))\n",
    "print(tagtuple_convert(pos_tag(word_tokenize(cdf[\"cause\"][1]))))\n",
    "print(tagtuple_convert(pos_tag(word_tokenize(cdf[\"effect\"][0]))))\n",
    "print(tagtuple_convert(pos_tag(word_tokenize(cdf[\"effect\"][1]))))\n",
    "ntcause = []\n",
    "ntcause.append(tagtuple_convert(pos_tag(word_tokenize(cdf[\"cause\"][1]))))\n",
    "ntcause.append(tagtuple_convert(pos_tag(word_tokenize(cdf[\"cause\"][0]))))\n",
    "nteffect = []\n",
    "nteffect.append(tagtuple_convert(pos_tag(word_tokenize(cdf[\"effect\"][1]))))\n",
    "nteffect.append(tagtuple_convert(pos_tag(word_tokenize(cdf[\"effect\"][0]))))\n",
    "ntdata = {\"cause\":ntcause,\"effect\":nteffect,\"doc_ind\":[5,7],\"sentence_ind\":[\"NA\",\"NA\"]}\n",
    "nt = pd.DataFrame(data = ntdata,index = [143,144])\n",
    "#print(nt)\n",
    "newtagdf = tagdf\n",
    "newtagdf = newtagdf.append(nt)\n",
    "print(newtagdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newlemmacdf = pd.DataFrame(data = {\"cause\":list(lemmacdf[\"cause\"]),\n",
    "                                  \"effect\":list(lemmacdf[\"effect\"]),\n",
    "                                  \"doc_ind\":list(lemmacdf[\"doc_ind\"]),\n",
    "                                  \"sentence_ind\":list(lemmacdf[\"sentence_ind\"])},\n",
    "                          index = range(145))\n",
    "nouns = pairnouns(picknouns(newlemmacdf,newtagdf))\n",
    "print(nouns)\n",
    "hypers = hypernyms(nouns)\n",
    "hypers.to_csv(r'./Documents/PhD paper 2/data/hypersbush.csv')\n",
    "print(hypers)\n",
    "verbs = pickverbs(newlemmacdf,newtagdf)\n",
    "classes = verbclass(verbs)\n",
    "classes.to_csv(r'./Documents/PhD paper 2/data/classesbush.csv')\n",
    "newlemmacdf.to_csv(r'./Documents/PhD paper 2/data/filterbush.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### combine results from caevo to extracted causal events\n",
    "## caevodf has a column of document index(use xmftodf to each document and concatenate them)\n",
    "## docindex = [1:...]\n",
    "## this function not tested yet\n",
    "\n",
    "def orderedevent(caevodf,filterdf,lemmacaevodf):\n",
    "    ## lemmatize the caevodf:\n",
    "    i = 0\n",
    "    for i in list(caevodf.index):\n",
    "        lemmacaevodf[\"cause\"][i] = lemmatizer.lemmatize(caevodf[\"cause\"][i],\"v\")\n",
    "        lemmacaevodf[\"effect\"][i] = lemmatizer.lemmatize(caevodf[\"effect\"][i],\"v\")\n",
    "    addcause = []\n",
    "    addeffect = []\n",
    "    adddoc = []\n",
    "    addsent = []\n",
    "    \n",
    "    docindex = np.unique(filterdf[\"doc_ind\"])\n",
    "    ## make a string of filterdf cause, effect with only verbs\n",
    "    for j in docindex:\n",
    "        tempdf = filterdf[filterdf[\"doc_ind\"] == j]\n",
    "        temppair = lemmacaevodf[lemmacaevodf[\"doc_ind\"] == j]\n",
    "       # k = 0\n",
    "        \n",
    "        if len(temppair[\"doc_ind\"]) > 0:\n",
    "            for k in list(temppair.index):\n",
    "                add = False\n",
    "                for h in list(tempdf.index):\n",
    "                    exist1 = temppair[\"cause\"][k] in tempdf[\"cause\"][h]\n",
    "                    exist2 = temppair[\"cause\"][k] in tempdf[\"effect\"][h]\n",
    "                    exist3 = temppair[\"effect\"][k] in tempdf[\"cause\"][h]\n",
    "                    exist4 = temppair[\"effect\"][k] in tempdf[\"effect\"][h]\n",
    "                    cond1 = exist1 and exist3\n",
    "                    cond2 = exist1 and exist4\n",
    "                    cond3 = exist2 and exist3\n",
    "                    cond4 = exist2 and exist4\n",
    "                    existed = cond1 or cond2 or cond3 or cond4\n",
    "                    add = add or existed\n",
    "                if add == False: ## if any of them is true, then the pair already exist\n",
    "                    ## we should not change the existed pairs, otherwise we add this pair\n",
    "                    addcause.append(temppair[\"cause\"][k])\n",
    "                    addeffect.append(temppair[\"effect\"][k])\n",
    "                    adddoc.append(j)\n",
    "                    addsent.append(\"NA\")\n",
    "    adddata = {\"cause\":addcause,\"effect\":addeffect,\"doc_ind\":adddoc,\"sentence_ind\":addsent}\n",
    "    adddf = pd.DataFrame(data = adddata)\n",
    "\n",
    "    outdf = filterdf.append(adddf)\n",
    "    return outdf\n",
    "                    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filterdf = pd.DataFrame(data={\"cause\":df[\"cause\"],\"effect\":df[\"effect\"],\"doc_ind\":df[\"doc_ind\"],\"sentence_ind\":df[\"sentence_ind\"]})\n",
    "ind = 0\n",
    "for text in df[\"cause\"]:\n",
    "    filterdf[\"cause\"][ind] = rm_stop(text)\n",
    "    ind += 1\n",
    "ind = 0\n",
    "for text in df[\"effect\"]:\n",
    "    filterdf[\"effect\"][ind] = rm_stop(text)\n",
    "    ind += 1\n",
    "print(filterdf)\n",
    "print(df)\n",
    "#print([pos_tag(word_tokenize(df[\"effect\"][0]))[i] for i in subset_sentence(filterdf[\"cause\"][0],df[\"cause\"][0])])\n",
    "tagdf = pd.DataFrame(data={\"cause\":df[\"cause\"],\"effect\":df[\"effect\"],\"doc_ind\":df[\"doc_ind\"],\"sentence_ind\":df[\"sentence_ind\"]})\n",
    "tagdf = wntagset(tagdf,df,filterdf)\n",
    "print(tagdf)\n",
    "nouns = pairnouns(picknouns(filterdf,tagdf))\n",
    "print(nouns)\n",
    "hypers = hypernyms(nouns)\n",
    "print(hypers)\n",
    "verbs = pickverbs(filterdf,tagdf)\n",
    "classes = verbclass(verbs)\n",
    "print(classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
